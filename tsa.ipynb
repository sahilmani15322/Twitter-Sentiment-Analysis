{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from sys import argv\n",
    "\n",
    "#script, file_ = argv\n",
    "\n",
    "def processRow(row):\n",
    "\t\n",
    "\t\n",
    "\ttweet = row[2]\n",
    "\t#Lower case\n",
    "\ttweet.lower()\n",
    "\t#convert any url to URL\n",
    "\ttweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "\t#Convert any @Username to \"AT_USER\"\n",
    "\ttweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "\t#Remove additional white spaces\n",
    "\ttweet = re.sub('[\\s]+', ' ', tweet)\n",
    "\ttweet = re.sub('[\\n]+', ' ', tweet)\n",
    "\t#Remove not alphanumeric symbols white spaces\n",
    "\ttweet = re.sub(r'[^\\w]', ' ', tweet)\n",
    "\t#Replace #word with word\n",
    "\ttweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "\t#Remove :( or :)\n",
    "\ttweet = tweet.replace(':)','')\n",
    "\ttweet = tweet.replace(':(','')\n",
    "\t#trim\n",
    "\ttweet = tweet.strip('\\'\"')\n",
    "\n",
    "\trow[2] = tweet\n",
    "\n",
    "\treturn row\n",
    "\n",
    "# end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start replaceTwoOrMore\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "def getStopWordList(stopWordFile):\n",
    "\tstopwords = []\n",
    "\tstopwords.append(\"AT_USER\")\n",
    "\tstopwords.append(\"URL\")\n",
    "\n",
    "\twith open(stopWordFile, 'r') as f:\n",
    "\t\treader = csv.reader(f)\n",
    "\t\tfor w in reader:\n",
    "\n",
    "\t\t\tstopwords.append(w[0])\n",
    "\n",
    "\treturn stopwords\n",
    "#end\n",
    "\n",
    "\n",
    "def getFeatureVector(tweet, stopWordFile):\n",
    "\tfeatures = []\n",
    "\n",
    "\tstop_words = getStopWordList(stopWordFile)\n",
    "\n",
    "\twords = tweet.split()\n",
    "\tfor w in words:\n",
    "\n",
    "\t\tw = replaceTwoOrMore(w)\n",
    "\n",
    "\t\t#strip digits\n",
    "\t\tw = w.strip('0123456789')\n",
    "\n",
    "\t\t#strip punctuation\n",
    "\t\tw = w.strip('\\'\"!?,.')\n",
    "\n",
    "\t\tif (w == \"\"):\n",
    "\t\t\tcontinue\n",
    "\t\telif(w in stop_words):\n",
    "\t\t\t#print w\n",
    "\t\t\tcontinue\n",
    "\t\telse:\n",
    "\t\t\tfeatures.append(w.lower())\n",
    "\n",
    "\treturn features\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "501\n",
      "train on 800 instances, test on 202 instances\n",
      "Most Informative Features\n",
      "           contains free = True           positi : negati =    200.3 : 1.0\n",
      "          contains check = True           positi : negati =    200.3 : 1.0\n",
      "             contains bi = True           positi : negati =    199.7 : 1.0\n",
      "            contains thx = True           positi : negati =    199.7 : 1.0\n",
      "             contains do = True           positi : negati =    125.7 : 1.0\n",
      "          contains hurry = True           positi : negati =     81.7 : 1.0\n",
      "             contains hi = True           positi : negati =     39.9 : 1.0\n",
      "          contains wanna = True           positi : negati =     20.8 : 1.0\n",
      "            contains ios = True           negati : positi =     12.2 : 1.0\n",
      "            contains don = True           negati : positi =     11.7 : 1.0\n",
      "pos precision: 1.0\n",
      "pos recall: 0.693069306930693\n",
      "pos F-measure: 0.8187134502923976\n",
      "neg precision: 0.7651515151515151\n",
      "neg recall: 1.0\n",
      "neg F-measure: 0.8669527896995707\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import *\n",
    "\n",
    "\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import nltk.metrics\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "stop_words_file_name ='stopwords.txt'\n",
    "\n",
    "word_features = []\n",
    "\n",
    "def read_tweets(tweet_file):\n",
    "\n",
    "\tfeatures = []\n",
    "\ttweets = []\n",
    "\n",
    "\twith open(tweet_file,'r') as csv_file:\n",
    "\t\tcsv_reader = csv.reader(csv_file)\n",
    "\n",
    "\t\tfor l in csv_reader:\n",
    "\t\t\t\t\n",
    "\t\t\t\tif l[0] == \"TweetID\":\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\tnew_row = processRow(l)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif len(new_row) < 4:\n",
    "\t\t\t\t\tprint (\"Malformed Data\")\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\n",
    "\t\t\t\tfeatures = getFeatureVector(new_row[2], stop_words_file_name)\n",
    "\n",
    "\t\t\t\ttweets.append(\n",
    "\t\t\t \t\t( [f.strip(\"\\'\") for f in features],\n",
    "\t\t\t \t\t new_row[3]\n",
    "\t\t\t \t\t))\n",
    "\n",
    "\treturn tweets\n",
    "#end\n",
    "\n",
    "def extract_features(document):\n",
    "\tdocument_words = set(document)\n",
    "\tfeatures = {}\n",
    "\tfor w in word_features:\n",
    "\t\tfeatures['contains %s'% w] = (w in document_words)\n",
    "\treturn features\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t\n",
    "\tneg_tweets = read_tweets('dataset/iphone6-negative.csv')\n",
    "\tpos_tweets = read_tweets('dataset/iphone6-positive.csv')\n",
    "\n",
    "\tcutoff = 0\n",
    "\n",
    "\tprint(len(neg_tweets))\n",
    "\tprint(len(pos_tweets))\n",
    "\tif(len(neg_tweets) > len(pos_tweets)):\n",
    "\t\tcutoff = len(pos_tweets)*4/5\n",
    "\telse:\n",
    "\t\tcutoff = len(neg_tweets)*4/5\n",
    " \n",
    "\ttweets = neg_tweets[:int(cutoff)] + pos_tweets[:int(cutoff)]\n",
    "\ttest_tweets = neg_tweets[int(cutoff):] + pos_tweets[int(cutoff):]\n",
    "\t\n",
    "\tprint ('train on %d instances, test on %d instances' % (len(tweets), len(test_tweets)))\n",
    " \n",
    "\tall_words = []\n",
    "\twords_frequency = []\n",
    "\n",
    "\t#Get all the words\n",
    "\tfor (words, sentiment) in tweets:\n",
    "\t\tall_words.extend(words)\n",
    "\n",
    "\t#extract the features\n",
    "\twordlist = nltk.FreqDist(all_words)\n",
    "\tword_features = wordlist.keys()\n",
    "\n",
    "\ttraining_set = nltk.classify.apply_features(extract_features, tweets)\n",
    "\n",
    "    \n",
    "\t#train the classifier\n",
    "\tclassifier = NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "\trefsets  = { 'positive': set([]), 'negative':set([])}\n",
    "\ttestsets = { 'positive': set([]), 'negative':set([])}\n",
    "\n",
    "   \n",
    "\tclassifier.show_most_informative_features()\n",
    "    \n",
    "\tfor i, (feats, label) in enumerate(test_tweets):   \n",
    "\t\t#print(feats,label,classifier.classify(extract_features(feats)))\n",
    "\t\trefsets[label].add(i)\n",
    "\t\ttestsets[classifier.classify(extract_features(feats))].add(i)\n",
    "\n",
    "    \n",
    "\tfrom nltk.metrics import precision,recall,f_measure\n",
    "    \n",
    "\tprint ('pos precision:',precision(refsets['positive'], testsets['positive']))\n",
    "\tprint ('pos recall:', recall(refsets['positive'], testsets['positive']))\n",
    "\tprint ('pos F-measure:',f_measure(refsets['positive'], testsets['positive']))\n",
    "\tprint ('neg precision:',precision(refsets['negative'], testsets['negative']))\n",
    "\tprint ('neg recall:', recall(refsets['negative'], testsets['negative']))\n",
    "\tprint ('neg F-measure:',f_measure(refsets['negative'], testsets['negative']))\n",
    "\n",
    "\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
